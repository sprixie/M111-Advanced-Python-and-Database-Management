{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Web Scraping using Beautiful Soup</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The internet is an absolutely massive source of data, There is a lot of information out there, But It isnâ€™t always available in convenient CSV exports or easy-to-connect APIs.\n",
    "and the Web Scraping remain the only way for accessing to the data.\n",
    "\n",
    "In this Notebook weâ€™re going to cover how to do web scraping with Python from scratch using Beautiful Soup. weâ€™ll work through an actual web scraping project, focusing on weather data.\n",
    "\n",
    "## The Fundamentals of Web Scraping:\n",
    "### What is Web Scraping?\n",
    "\n",
    "Some websites offer data sets that are downloadable in CSV format, or accessible via an Application Programming Interface (API). But many websites with useful data donâ€™t offer these convenient options.\n",
    "\n",
    "Consider for example, The Weather Channel that provide national and local weather forecast for cities over the world. But the weather data isnâ€™t accessible as a CSV or via API. It has to be viewed on its website `weather.com`. \n",
    "\n",
    "![](../data/weather.com)\n",
    "\n",
    "In order to get and work with this data we need Web Scraping.\n",
    "\n",
    "Web scraping is an automatic method to obtain large amounts of data from websites.\n",
    "\n",
    "In this workshop weâ€™ll write some code that looks at the The *Weather Channel* site, grabs just the data we want to work with, and outputs it in the format we need.\n",
    "\n",
    "\n",
    "### How Does Web Scraping Work?\n",
    "When we scrape the web, we write code that sends a request to the server thatâ€™s hosting the page we specified. The server will return the source code â€” HTML, mostly â€” for the page (or pages) we requested. and we will look into specific element in this code and extract the content we are interested about.\n",
    "\n",
    "In short Web Scraping can be resumed into these steps:\n",
    "1. Request the content (source code) of a specific URL from the server\n",
    "2. Download the content that is returned\n",
    "3. Identify the elements of the page that are part of the table we want\n",
    "4. Extract and (if necessary) reformat those elements into a dataset we can analyze or use in whatever way we require.\n",
    "\n",
    "### Is Web Scraping Legal?\n",
    "Thereâ€™s not a cut-and-dry answer here. Some websites explicitly allow web scraping. Others explicitly forbid it. Many websites donâ€™t offer any clear guidance one way or the other.\n",
    "\n",
    "Before scraping any website, we should look for a terms and conditions page to see if there are explicit rules about scraping. If there are, we should follow them. If there are not, then it becomes more of a judgement call.\n",
    "\n",
    "\n",
    "### Web Scraping Best Practices:\n",
    "- Never scrape more frequently than you need to.\n",
    "- Consider caching the content you scrape so that itâ€™s only downloaded once.\n",
    "- Build pauses into your code between requests to keep you from overwhelming servers with too many requests too quickly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Components of a Web Page\n",
    "\n",
    "When we visit a web page, our web browser makes a request to a web server. This request is called a `GET` request, since weâ€™re getting files from the server. The server then sends back files that tell our browser how to render the page for us. These files will typically include:\n",
    "- HTML â€” the main content of the page.\n",
    "- CSS â€” used to add styling to make the page look nicer.\n",
    "- JS â€” Javascript files add interactivity to web pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML page structure\n",
    "\n",
    "**Hypertext Markup Language (HTML)** is the standard markup language for documents designed to be displayed in a web browser. HTML describes the structure of a web page and it can be used with **Cascading Style Sheets (CSS)** and a scripting language such as **JavaScript** to create interactive websites. HTML consists of a series of elements that \"tell\" to the browser how to display the content. Lastly, elements are represented by **tags**.\n",
    "\n",
    "Here are some tags:\n",
    "* `<!DOCTYPE html>` declaration defines this document to be HTML5.  \n",
    "* `<html>` element is the root element of an HTML page.  \n",
    "* `<div>` tag defines a division or a section in an HTML document. It's usually a container for other elements.\n",
    "* `<head>` element contains meta information about the document.  \n",
    "* `<title>` element specifies a title for the document.  \n",
    "* `<body>` element contains the visible page content.  \n",
    "* `<h1>` element defines a large heading.  \n",
    "* `<p>` element defines a paragraph.  \n",
    "* `<a>` element defines a hyperlink.\n",
    "\n",
    "HTML tags normally come in pairs like `<p>` and `</p>`. The first tag in a pair is the opening tag, the second tag is the closing tag. The end tag is written like the start tag, but with a slash inserted before the tag name.\n",
    "\n",
    "![](../data/tag.png)\n",
    "\n",
    "HTML has a tree-like ðŸŒ³ ðŸŒ² structure thanks to the **Document Object Model (DOM)**, a cross-platform and language-independent interface. Here's how a very simple HTML tree looks like.\n",
    "\n",
    "![](../data/dom_tree.gif)\n",
    "\n",
    "There are a lot of tags that we don't cover here, for a full list of tags, look [here](https://developer.mozilla.org/en-US/docs/Web/HTML/Element).\n",
    "\n",
    "**Class and ID**\n",
    "\n",
    "Before we move into actual web scraping, letâ€™s learn about the class and id properties. These special properties give HTML elements names, and make them easier to interact with when weâ€™re scraping.\n",
    "\n",
    "One element can have multiple classes, and a class can be shared between elements. Each element can only have one id, and an id can only be used once on a page. Classes and ids are optional, and not all elements will have them.\n",
    "\n",
    "### Creating a simple HTML page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\" dir=\"ltr\">\n",
    "<head>\n",
    "  <title>Intro to HTML</title>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "  <h1>Heading h1</h1>\n",
    "  <h2>Heading h2</h2>\n",
    "  <h3>Heading h3</h3>\n",
    "  <h4>Heading h4</h4>\n",
    "\n",
    "  <p>\n",
    "    That's a text paragraph. You can also <b>bold</b>, <mark>mark</mark>, <ins>underline</ins>, <del>strikethrough</del> and <i>emphasize</i> words.\n",
    "    You can also add links - here's one to <a href=\"https://en.wikipedia.org/wiki/Main_Page\">Wikipedia</a>.\n",
    "  </p>\n",
    "\n",
    "  <p>\n",
    "    This <br> is a paragraph <br> with <br> line breaks\n",
    "  </p>\n",
    "\n",
    "  <p style=\"color:red\">\n",
    "    Add colour to your paragraphs.\n",
    "  </p>\n",
    "\n",
    "  <p>Unordered list:</p>\n",
    "  <ul>\n",
    "    <li>Python</li>\n",
    "    <li>R</li>\n",
    "    <li>Julia</li>\n",
    "  </ul>\n",
    "\n",
    "  <p>Ordered list:</p>\n",
    "  <ol>\n",
    "    <li>Data collection</li>\n",
    "    <li>Exploratory data analysis</li>\n",
    "    <li>Data analysis</li>\n",
    "    <li>Policy recommendations</li>\n",
    "  </ol>\n",
    "  <hr>\n",
    "\n",
    "  <!-- This is a comment -->\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping with `requests` and `BeautifulSoup`\n",
    "\n",
    "We will use `requests` and `BeautifulSoup` to access and scrape the content of `weather.com`\n",
    "\n",
    "### What is `BeautifulSoup`?\n",
    "\n",
    "It is a Python library for pulling data out of HTML and XML files. It provides methods to navigate the document's tree structure that we discussed before and scrape its content.\n",
    "\n",
    "### Our pipeline\n",
    "![](data/scrape-pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "page = requests.get(\"http://istodaytietuesday.com/\")\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the BeautifulSoup library to parse this document, and extract the text from the p tag.\n",
    "\n",
    "We first have to import the library, and create an instance of the BeautifulSoup class to parse our document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now print out the HTML content of the page, formatted nicely, using the prettify method on the BeautifulSoup object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some simple ways to navigate that data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title\n",
    "# <title>Is Today Tie Tuesday?</title>\n",
    "\n",
    "soup.title.name\n",
    "# 'title'\n",
    "\n",
    "soup.title.string\n",
    "# 'Is Today Tie Tuesday?'\n",
    "\n",
    "soup.title.parent.name\n",
    "# 'head'\n",
    "\n",
    "soup.link\n",
    "# <link href=\"css/normalize.min.css\" rel=\"stylesheet\"/>\n",
    "\n",
    "soup.div\n",
    "# <div class=\"main-container\">\n",
    "# <div class=\"answer\">\n",
    "# <h1 class=\"answer-text\"></h1>\n",
    "# </div>\n",
    "# </div>\n",
    "\n",
    "soup.div['class']\n",
    "# ['main-container']\n",
    "\n",
    "soup.find_all('div')\n",
    "# [<div class=\"main-container\">\n",
    "#  <div class=\"answer\">\n",
    "#  <h1 class=\"answer-text\"></h1>\n",
    "#  </div>\n",
    "#  </div>,\n",
    "#  <div class=\"answer\">\n",
    "#  <h1 class=\"answer-text\"></h1>\n",
    "#  </div>]\n",
    "\n",
    "soup.find(id=\"link3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data-testid=\"DetailsSummary\"\n",
    "contain_attr = soup.find_all(attrs={\"value\" : \"something\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using_class = test.find(class_= \"class_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find by element and class name\n",
    "element = soup.find(\"span\",  {\"class\": \"main_container\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the nth child \n",
    "element.select_one(\":nth-child(3)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "9258da9d89097b7938e69cfd13921ef5e79cc49d535702d66f1d23fd82f99e8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
